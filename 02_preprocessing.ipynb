{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Полная предобработка для всех источников"
      ],
      "metadata": {
        "id": "sJ3bFIdGkm_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Установка зависимостей и загрузка моделей"
      ],
      "metadata": {
        "id": "dmzEB4Llk2pT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d23f14-3d2d-4d3d-ab43-036d8586c095",
        "id": "1smVOJLqk2pU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=d88f083c5dfdbeff03a0d601af5d33353420d6290c3c1e6e14f77bd14fd7d8c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2 nltk tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Импорт библиотек"
      ],
      "metadata": {
        "id": "UzHf7jtVk2pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "import nltk\n",
        "import pymorphy2\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "\n",
        "# Заплатка для совместимости pymorphy2 с Python 3.11\n",
        "import inspect\n",
        "from collections import namedtuple\n",
        "\n",
        "if not hasattr(inspect, 'getargspec'):\n",
        "    ArgSpec = namedtuple('ArgSpec', ['args', 'varargs', 'keywords', 'defaults'])\n",
        "\n",
        "    def getargspec(func):\n",
        "        spec = inspect.getfullargspec(func)\n",
        "        return ArgSpec(\n",
        "            args=spec.args,\n",
        "            varargs=spec.varargs,\n",
        "            keywords=spec.varkw,\n",
        "            defaults=spec.defaults\n",
        "        )\n",
        "\n",
        "    inspect.getargspec = getargspec"
      ],
      "metadata": {
        "id": "v_S1SFLAk2pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Монтирование Google Диска"
      ],
      "metadata": {
        "id": "4AIExqTfk2pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee2c195a-f526-41ef-b8d9-7eceb3115d0f",
        "id": "pSPakrRfk2pV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Загрузка моделей и стоп-слов"
      ],
      "metadata": {
        "id": "zAp1IP05k2pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Части речи, которые считаем \"мусорными\"\n",
        "bad_pos = {\"PRON\", \"ADV\", \"AUX\", \"PART\", \"DET\", \"SCONJ\", \"CCONJ\", \"INTJ\", \"ADP\", \"NUM\", \"SYM\", \"X\", \"PUNCT\", \"SPACE\"}\n",
        "\n",
        "# Загружаем пользовательские стоп-слова\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "russian_stopwords = set(stopwords.words(\"russian\"))\n",
        "\n",
        "custom_stopwords_path = '/content/drive/MyDrive/TextScope/custom_stopwords.txt'\n",
        "os.makedirs(os.path.dirname(custom_stopwords_path), exist_ok=True)\n",
        "\n",
        "if not os.path.exists(custom_stopwords_path):\n",
        "    with open(custom_stopwords_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"# Добавьте сюда свои стоп-слова по одному на строку.\\n\")\n",
        "        f.write(\"привет\\nздравствуйте\\nстатья\\nдоклад\\n\")\n",
        "\n",
        "with open(custom_stopwords_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line and not line.startswith(\"#\"):\n",
        "            russian_stopwords.add(line.lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9673b86-f0b0-4091-d31c-f905bf38d894",
        "id": "Jlziuowbk2pW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Вспомогательные функции для предобработки"
      ],
      "metadata": {
        "id": "_tXtVFO-k2pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация pymorphy2\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def full_preprocess(text):\n",
        "    text = re.sub(r'[^а-яА-ЯёЁ\\s]', '', str(text).lower())\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        parsed = morph.parse(token)[0]\n",
        "        lemma = parsed.normal_form\n",
        "        if parsed.tag.POS not in bad_pos and lemma not in russian_stopwords:\n",
        "            lemmas.append(lemma)\n",
        "    return ' '.join(lemmas)"
      ],
      "metadata": {
        "id": "XmiQ-SAyk2pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Функция для извлечения IDF-стоп-слов"
      ],
      "metadata": {
        "id": "AxPnFR1zk2pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_idf_stopwords_streamed(corpus, idf_threshold=6.0, max_df_ratio=0.98):\n",
        "    total_docs = 0\n",
        "    doc_freq = defaultdict(int)\n",
        "    term_freq = Counter()\n",
        "    for text in tqdm(corpus, desc=\"Вычисление IDF\", leave=False):\n",
        "        tokens = nltk.word_tokenize(text)\n",
        "        lemmas = set()\n",
        "        for token in tokens:\n",
        "            parsed = morph.parse(token)[0]\n",
        "            lemma = parsed.normal_form\n",
        "            if parsed.tag.POS not in bad_pos:\n",
        "                lemmas.add(lemma)\n",
        "                term_freq[lemma] += 1\n",
        "        for lemma in lemmas:\n",
        "            doc_freq[lemma] += 1\n",
        "        total_docs += 1\n",
        "\n",
        "    return {word for word, df in doc_freq.items()\n",
        "            if math.log((total_docs + 1) / (df + 1)) + 1 < idf_threshold or df / total_docs > max_df_ratio}"
      ],
      "metadata": {
        "id": "viDNBNl9k2pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Основная функция обработки CSV-файла"
      ],
      "metadata": {
        "id": "3ZWkF0HEk2pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_fully_processed(input_path, output_path):\n",
        "    \"\"\"Проверяет, что выходной файл содержит метку .done, что означает полную обработку\"\"\"\n",
        "    return os.path.exists(f\"{output_path}.done\")\n",
        "\n",
        "def save_processed_info(chunk_count, output_path):\n",
        "    \"\"\"Сохраняет количество обработанных чанков\"\"\"\n",
        "    with open(f\"{output_path}.progress\", 'w') as f:\n",
        "        f.write(str(chunk_count))\n",
        "\n",
        "def get_last_processed_chunk(output_path):\n",
        "    \"\"\"Считывает количество уже обработанных чанков, если есть\"\"\"\n",
        "    progress_file = f\"{output_path}.progress\"\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            return int(f.read().strip())\n",
        "    return 0"
      ],
      "metadata": {
        "id": "9sL_tJz5k2pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_csv(file_path, output_path, text_column, chunksize=5000):\n",
        "    if is_fully_processed(file_path, output_path):\n",
        "        print(f\"Файл уже полностью обработан: {output_path} — пропущен.\\n\")\n",
        "        return\n",
        "\n",
        "    print(f\"Обработка файла: {file_path}\")\n",
        "    all_lemmas = []\n",
        "    all_docs = []\n",
        "    is_first_chunk = not os.path.exists(output_path)\n",
        "\n",
        "    total_rows = 0\n",
        "    total_chunks = 0\n",
        "    for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
        "        total_chunks += 1\n",
        "        total_rows += len(chunk)\n",
        "    print(f\"Всего строк: {total_rows}, всего чанков: {total_chunks}\")\n",
        "\n",
        "    processed_chunks = get_last_processed_chunk(output_path)\n",
        "    print(f\"Последний обработанный чанк: {processed_chunks}/{total_chunks}\")\n",
        "\n",
        "    reader = pd.read_csv(file_path, chunksize=chunksize)\n",
        "    with tqdm(total=total_chunks, initial=processed_chunks, unit='chunk', desc=os.path.basename(file_path)) as pbar:\n",
        "        for i, chunk in enumerate(reader):\n",
        "            idx = i + processed_chunks\n",
        "            if idx < processed_chunks:\n",
        "                pbar.update(1); continue\n",
        "            if idx >= total_chunks:\n",
        "                break\n",
        "\n",
        "            if text_column not in chunk.columns or chunk.empty:\n",
        "                pbar.update(1); continue\n",
        "            chunk.dropna(subset=[text_column], inplace=True)\n",
        "            chunk.drop_duplicates(subset=[text_column], inplace=True)\n",
        "\n",
        "            if 'gnews_data.csv' in file_path and 'date' in chunk.columns:\n",
        "                chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce', utc=True)\n",
        "                chunk['year'] = chunk['date'].dt.year\n",
        "                chunk['date'] = chunk['date'].dt.strftime('%Y-%m-%d')\n",
        "            elif 'cyberleninka_data.csv' in file_path and 'year' in chunk.columns:\n",
        "                chunk['date'] = pd.to_datetime(chunk['year'].astype(str) + '-01-01', errors='coerce')\n",
        "                chunk['year'] = chunk['date'].dt.year\n",
        "                chunk['date'] = chunk['date'].dt.strftime('%Y-%m-%d')\n",
        "            elif 'date' in chunk.columns:\n",
        "                chunk['date'] = pd.to_datetime(chunk['date'], errors='coerce', utc=True)\n",
        "                chunk['year'] = chunk['date'].dt.year\n",
        "\n",
        "            if 'cyberleninka_data.csv' in file_path and 'author' in chunk.columns:\n",
        "                chunk['author'] = chunk['author'].astype(str).str.replace(r'\\n', ' ', regex=True)\n",
        "\n",
        "            chunk[text_column] = chunk[text_column].apply(full_preprocess)\n",
        "            chunk = chunk[chunk[text_column].str.strip().astype(bool)]\n",
        "\n",
        "            docs_str = chunk[text_column].tolist()\n",
        "            all_docs.extend(docs_str)\n",
        "            for doc in docs_str:\n",
        "                all_lemmas.extend(doc.split())\n",
        "\n",
        "            chunk.to_csv(output_path, mode='a', header=is_first_chunk, index=False)\n",
        "            is_first_chunk = False\n",
        "            save_processed_info(idx + 1, output_path)\n",
        "            pbar.update(1)\n",
        "\n",
        "    idf_stopwords = get_idf_stopwords_streamed(all_docs)\n",
        "    russian_stopwords.update(idf_stopwords)\n",
        "    with open(custom_stopwords_path, 'a', encoding='utf-8') as f:\n",
        "        for w in sorted(idf_stopwords):\n",
        "            f.write(w + \"\\n\")\n",
        "\n",
        "    freq_clean = Counter(all_lemmas)\n",
        "    rare = {w for w, f in freq_clean.items() if f == 1}\n",
        "\n",
        "    df = pd.read_csv(output_path)\n",
        "    df[text_column] = df[text_column].apply(\n",
        "    lambda t: ' '.join(w for w in t.split() if w not in rare and len(w) >= 4)\n",
        "    )\n",
        "    df.to_csv(output_path, index=False, quoting=csv.QUOTE_ALL, quotechar='\"')\n",
        "\n",
        "    base = os.path.splitext(os.path.basename(file_path))[0]\n",
        "    freq_path = f\"/content/drive/MyDrive/TextScope/{base}_freq_dict.txt\"\n",
        "    with open(freq_path, 'w', encoding='utf-8') as f:\n",
        "        for word, cnt in sorted(freq_clean.items(), key=lambda x: x[1], reverse=True):\n",
        "            if cnt > 1:\n",
        "                f.write(f\"{word}:{cnt}\\n\")\n",
        "    print(f\"Частотный словарь сохранен: {freq_path}\")\n",
        "\n",
        "    with open(f\"{output_path}.done\", 'w', encoding='utf-8') as f:\n",
        "        f.write(\"Готово\\n\")\n",
        "    print(f\"Файл обработан: {output_path}\\n\")"
      ],
      "metadata": {
        "id": "nxBjBykCk2pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Запуск обработки всех файлов"
      ],
      "metadata": {
        "id": "d0yjk9j-k2pY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Путь к CSV-файлам\n",
        "csv_files = [\n",
        "    {'path': '/content/drive/MyDrive/TextScope/vk_data.csv', 'output': '/content/drive/MyDrive/TextScope/processed_vk_data.csv', 'text_column': 'text'},\n",
        "    {'path': '/content/drive/MyDrive/TextScope/gnews_data.csv', 'output': '/content/drive/MyDrive/TextScope/processed_gnews_data.csv', 'text_column': 'text'},\n",
        "    {'path': '/content/drive/MyDrive/TextScope/cyberleninka_data.csv', 'output': '/content/drive/MyDrive/TextScope/processed_cyberleninka_data.csv', 'text_column': 'text'}\n",
        "]\n",
        "\n",
        "for file_info in csv_files:\n",
        "    preprocess_csv(file_info['path'], file_info['output'], file_info['text_column'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d46c468-ecd7-4bbd-a00c-203240dbea73",
        "id": "Ozxjl6J-k2pY"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обработка файла: /content/drive/MyDrive/TextScope/vk_data.csv\n",
            "Всего строк: 42395, всего чанков: 9\n",
            "Последний обработанный чанк: 0/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "vk_data.csv: 100%|██████████| 9/9 [27:43<00:00, 184.85s/chunk]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частотный словарь сохранен: /content/drive/MyDrive/TextScope/vk_data_freq_dict.txt\n",
            "Топ-50 частых слов сохранен: /content/drive/MyDrive/TextScope/vk_data_top_words.txt\n",
            "Файл обработан: /content/drive/MyDrive/TextScope/processed_vk_data.csv\n",
            "\n",
            "Обработка файла: /content/drive/MyDrive/TextScope/gnews_data.csv\n",
            "Всего строк: 16504, всего чанков: 4\n",
            "Последний обработанный чанк: 0/4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "gnews_data.csv: 100%|██████████| 4/4 [20:19<00:00, 304.78s/chunk]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частотный словарь сохранен: /content/drive/MyDrive/TextScope/gnews_data_freq_dict.txt\n",
            "Топ-50 частых слов сохранен: /content/drive/MyDrive/TextScope/gnews_data_top_words.txt\n",
            "Файл обработан: /content/drive/MyDrive/TextScope/processed_gnews_data.csv\n",
            "\n",
            "Обработка файла: /content/drive/MyDrive/TextScope/cyberleninka_data.csv\n",
            "Всего строк: 22639, всего чанков: 5\n",
            "Последний обработанный чанк: 0/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "cyberleninka_data.csv: 100%|██████████| 5/5 [2:47:07<00:00, 2005.54s/chunk]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частотный словарь сохранен: /content/drive/MyDrive/TextScope/cyberleninka_data_freq_dict.txt\n",
            "Топ-50 частых слов сохранен: /content/drive/MyDrive/TextScope/cyberleninka_data_top_words.txt\n",
            "Файл обработан: /content/drive/MyDrive/TextScope/processed_cyberleninka_data.csv\n",
            "\n"
          ]
        }
      ]
    }
  ]
}